{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "import numpy\n",
    "import math\n",
    "import numbers\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianSmoothing(nn.Module):\n",
    "    \"\"\"\n",
    "    Apply gaussian smoothing on a\n",
    "    1d, 2d or 3d tensor. Filtering is performed seperately for each channel\n",
    "    in the input using a depthwise convolution.\n",
    "    Arguments:\n",
    "        channels (int, sequence): Number of channels of the input tensors. Output will\n",
    "            have this number of channels as well.\n",
    "        kernel_size (int, sequence): Size of the gaussian kernel.\n",
    "        sigma (float, sequence): Standard deviation of the gaussian kernel.\n",
    "        dim (int, optional): The number of dimensions of the data.\n",
    "            Default value is 2 (spatial).\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, kernel_size, sigma, dim=2):\n",
    "        super(GaussianSmoothing, self).__init__()\n",
    "        if isinstance(kernel_size, numbers.Number):\n",
    "            kernel_size = [kernel_size] * dim\n",
    "        if isinstance(sigma, numbers.Number):\n",
    "            sigma = [sigma] * dim\n",
    "\n",
    "        # The gaussian kernel is the product of the\n",
    "        # gaussian function of each dimension.\n",
    "        kernel = 1\n",
    "        meshgrids = torch.meshgrid(\n",
    "            [\n",
    "                torch.arange(size, dtype=torch.float32)\n",
    "                for size in kernel_size\n",
    "            ]\n",
    "        )\n",
    "        for size, std, mgrid in zip(kernel_size, sigma, meshgrids):\n",
    "            mean = (size - 1) / 2\n",
    "            kernel *= 1 / (std * math.sqrt(2 * math.pi)) * \\\n",
    "                      torch.exp(-((mgrid - mean) / std) ** 2 / 2)\n",
    "\n",
    "        # Make sure sum of values in gaussian kernel equals 1.\n",
    "        kernel = kernel / torch.sum(kernel)\n",
    "\n",
    "        # Reshape to depthwise convolutional weight\n",
    "        kernel = kernel.view(1, 1, *kernel.size())\n",
    "        kernel = kernel.repeat(channels, *[1] * (kernel.dim() - 1))\n",
    "\n",
    "        self.register_buffer('weight', kernel)\n",
    "        self.groups = channels\n",
    "\n",
    "        if dim == 1:\n",
    "            self.conv = F.conv1d\n",
    "        elif dim == 2:\n",
    "            self.conv = F.conv2d\n",
    "        elif dim == 3:\n",
    "            self.conv = F.conv3d\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                'Only 1, 2 and 3 dimensions are supported. Received {}.'.format(dim)\n",
    "            )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Apply gaussian filter to input.\n",
    "        Arguments:\n",
    "            input (torch.Tensor): Input to apply gaussian filter on.\n",
    "        Returns:\n",
    "            filtered (torch.Tensor): Filtered output.\n",
    "        \"\"\"\n",
    "        return self.conv(input, weight=self.weight, groups=self.groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class akCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(akCNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(3, 128, 3, padding=1),nn.ELU())\n",
    "        # modify kernel size here\n",
    "        self.smoothing = GaussianSmoothing(128, kernel_size=3, sigma=1)\n",
    "\n",
    "        self.stack1 = nn.Sequential(\n",
    "        nn.Conv2d(128, 128, 3),\n",
    "        nn.ELU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Dropout2d(p=0.1)\n",
    "        )\n",
    "    \n",
    "        self.stack2 = nn.Sequential(\n",
    "        nn.Conv2d(128, 256, 3, padding=1),\n",
    "        nn.ELU(),\n",
    "        nn.Conv2d(256, 256, 3),\n",
    "        nn.ELU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Dropout2d(p=0.25)\n",
    "        )\n",
    "        \n",
    "        self.stack3 = nn.Sequential(\n",
    "        nn.Conv2d(256, 512, 3, padding=1),\n",
    "        nn.ELU(),\n",
    "        nn.Conv2d(512, 512, 3),\n",
    "        nn.ELU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Dropout2d(p=0.5)\n",
    "        )\n",
    "    \n",
    "        self.last = nn.Sequential(\n",
    "        nn.Linear(2048,1024),\n",
    "        nn.ELU(),\n",
    "        nn.Dropout2d(p=0.5),\n",
    "        nn.Linear(1024,100),\n",
    "        nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "        \n",
    "    def forward(self, x, verbose=False):\n",
    "        x = self.conv1(x)\n",
    "#         if self.training and self.sigma != 0:\n",
    "#             scale = self.sigma * x\n",
    "#             sampled_noise = self.noise.repeat(*x.size()).normal_() * scale\n",
    "#         x = x + sampled_noise\n",
    "        x = self.smoothing(x)\n",
    "        x = self.stack1(x)\n",
    "        x = self.stack2(x)\n",
    "        x = self.stack3(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.last(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = []\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch, log_interval = 100):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    # Loop through data points\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):    \n",
    "        # Send data and target to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Zero out the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # Pass data through model\n",
    "        output = model(data)\n",
    "        # Compute the negative log likelihood loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # Backpropagate loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Make a step with the optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print loss (uncomment lines below once implemented)\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "# Define test method\n",
    "def test(model, device, test_loader):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    # Variable for the total loss \n",
    "    test_loss = 0\n",
    "    # Counter for the correct predictions\n",
    "    num_correct = 0\n",
    "    \n",
    "    # don't need autograd for eval\n",
    "    with torch.no_grad():\n",
    "        # Loop through data points\n",
    "        for data, target in test_loader:\n",
    "            # Send data to device\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # Pass data through model\n",
    "            output = model(data)\n",
    "            # Compute the negative log likelihood loss with reduction='sum' and add to total test_loss\n",
    "            # sum losses over minibatch\n",
    "            test_loss += F.nll_loss(output, target, reduction = 'sum').item()\n",
    "            # Get predictions from the model for each data point\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability                                                                 \n",
    "                    \n",
    "            # Add number of correct predictions to total num_correct \n",
    "            num_correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "    \n",
    "    # Compute the average test_loss\n",
    "    avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "    \n",
    "    # Print loss (uncomment lines below once implemented)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_test_loss, num_correct, len(test_loader.dataset),\n",
    "        100. * num_correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download data\n",
    "trans = transforms.Compose([transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "                            ])\n",
    "cifar100Train = datasets.CIFAR100('data', train=True, transform=trans, download=True)\n",
    "trainSampler = RandomSampler(cifar100Train)\n",
    "train_loader = DataLoader(cifar100Train, batch_size=32, sampler=trainSampler)\n",
    "cifar100test = datasets.CIFAR100('data', train=False, transform=trans, download=True)\n",
    "testSampler = RandomSampler(cifar100test)\n",
    "test_loader = DataLoader(cifar100test, batch_size=32, sampler=testSampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test method\n",
    "def test(model, device, test_loader):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    # Variable for the total loss \n",
    "    test_loss = 0\n",
    "    # Counter for the correct predictions\n",
    "    num_correct = 0\n",
    "    \n",
    "    # don't need autograd for eval\n",
    "    with torch.no_grad():\n",
    "        # Loop through data points\n",
    "        for data, target in test_loader:        \n",
    "            # Send data to device\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # Pass data through model\n",
    "            output = model(data)\n",
    "            \n",
    "            # Compute the negative log likelihood loss with reduction='sum' and add to total test_loss\n",
    "            # sum losses over minibatch\n",
    "            test_loss += F.nll_loss(output, target, reduction = 'sum').item()\n",
    "            \n",
    "            # Get predictions from the model for each data point\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability                                                                 \n",
    "                    \n",
    "            # Add number of correct predictions to total num_correct \n",
    "            num_correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "    \n",
    "    # Compute the average test_loss\n",
    "    avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "    \n",
    "    # Print loss (uncomment lines below once implemented)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_test_loss, num_correct, len(test_loader.dataset),\n",
    "        100. * num_correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define model and sent to device\n",
    "'''\n",
    "    sigma (float, optional): relative standard deviation used to generate the\n",
    "    noise. Relative means that it will be multiplied by the magnitude of\n",
    "    the value your are adding the noise to. This means that sigma can be\n",
    "    the same regardless of the scale of the vector.\n",
    "'''\n",
    "            \n",
    "model = akCNN()\n",
    "\n",
    "# Optimizer: SGD with learning rate of 1e-2 and momentum of 0.5\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.5)\n",
    "\n",
    "# Training loop with 10 epochs\n",
    "for epoch in range(1, 10 + 1):\n",
    "    # Train model\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    break\n",
    "    # Test model\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0100, 0.0102, 0.0099,  ..., 0.0098, 0.0097, 0.0101],\n",
       "        [0.0097, 0.0099, 0.0099,  ..., 0.0101, 0.0102, 0.0099],\n",
       "        [0.0099, 0.0102, 0.0100,  ..., 0.0098, 0.0098, 0.0099],\n",
       "        ...,\n",
       "        [0.0100, 0.0102, 0.0099,  ..., 0.0097, 0.0102, 0.0102],\n",
       "        [0.0100, 0.0102, 0.0101,  ..., 0.0099, 0.0100, 0.0100],\n",
       "        [0.0101, 0.0101, 0.0100,  ..., 0.0098, 0.0102, 0.0101]],\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fakeInputs = torch.rand(16, 3, 32, 32).to(device)\n",
    "net = akCNN()\n",
    "net.to(device)\n",
    "net(fakeInputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
